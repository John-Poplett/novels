---
title: "Predicting the Success of Literary Text Based on a glmnet Model"
author: "John Poplett"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{glmnet}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: novels.bib
---

```{r options, include = FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```
```{r, echo=FALSE}
op = options(width = 80, str = strOptions(strict.width = "cut"))
```

# Introduction
This vignette uses text2vec and Stonybrook's novels dataset to create a classification and regression model to predict the likelihood of success for novels and other literary texts. The dataset and various approaches to creating a classifier are discussed in the original article "*Success with Style*" [@ashok_success_2013].

The authors of the original study chose to publish the raw data used for their dataset but did not publish the source code to their classifier. In the original study, the author's built a classifier using LibLinear SVM. They used a variety of approaches to transform text into "features" suitable for training a classifier. In this example, we make use of text2vec which, as its name implies, provides the tools to transform text into feature vectors.


```{r setup, eval=TRUE, message=FALSE}
library(dplyr)
library(novels)
library(data.table)
library(text2vec)
library(glmnet)
library(tools)
library(knitr)
library(kableExtra)
```
# The "novels" dataset
The novels dataset consists of `r dim(novels)[1]` entries. Some of the entries refer
to the same text file, e.g. in the case where a literary work belongs in more than one category (fiction and short story). For training, it's important that we only train on a given text once. In the entire dataset there are `r sum(novels %>% select(file.name) %>% duplicated)` duplicates.

Also, author's may have multiple works in the dataset. While we have a total of `r dim(novels)[1]` entries in the dataset, there are fewer authors `r length(unique(novels$author))`.

# Create training and test sets
```{r split, eval=TRUE, message=FALSE}
# Make response value an integer for AUC computation. Test to prevent
# confusion when rerunning code chunks.
if(class(novels$response) == 'factor') {
  stopifnot(novels$response == 'success' || novels$response == 'failure')
  novels$response <- as.integer(novels$response == 'success')
}
novels = copy(novels::novels)
setDT(novels)
set.seed(2019L)
all_ids = unique(novels$author)
indices <- createSplit(novels)
train = novels[indices$train]
test = novels[indices$test]
```

# Feature extraction
Create a text2vec iterator, vocabulary, vectorizer and document text matrix (dtm). The dtm represents the novels data set as trainable features.
```{r document-text-matrix, eval = TRUE}
it <- novels::getIterator(train$text, train$title)
vocab <- novels::text2vocab(it, length(train$text))
vectorizer = vocab_vectorizer(vocab)
dtm = create_dtm(it, vectorizer)
```

# Normalization
Transform the dtm to an idf to normalize the data and make it machine learning friendly.
```{r tfidf, eval = TRUE}
# define tfidf model
tfidf = TfIdf$new()
# fit model to train data and transform train data with fitted model
dtm_tfidf = fit_transform(dtm, tfidf)
```
```{r, echo=FALSE}
rm(dtm)
```

# Build classifier
Build a classifier using a generalized linear model (glm) with cross-validation (cv).
```{r classifier, eval = TRUE}
glmnet_classifier = cv.glmnet(x = dtm_tfidf, y = train$response,
                              family = 'binomial',
                              alpha = 0.1,
                              type.measure = "auc",
                              nfolds = 5,
                              thresh = 1e-5,
                              maxit = 1e4)
```

# Display AUC for the resulting model
Area under the curve (AUC) is a time-honored approach to evaluating the effectiveness of a model. Here we use the *glmnet* package to plot the AUC for different values of the control variable "lambda".
```{r plot-auc}
plot(glmnet_classifier)
```
# Evaluate test set
To understand the quality of our classifier, we want the model's best AUC result for both training and test data sets. We want AUC scores as close to 1 as possible, ideally over .9.

Although the test set rarely exceeds the training set results, we want a test set AUC result that is close to the training set AUC.
```{r eval-test-set}
classifier <- createPredictor(vectorizer, tfidf, glmnet_classifier)
test_results <- classifier(test, type = "response")
test_auc <- glmnet:::auc(test$response, test_results$prediction)
train_auc <- max(glmnet_classifier$cvm)
```
The classifier produces an AUC of `r sprintf("%02.2f", train_auc * 100)` on training data and an AUC of `r sprintf("%02.2f", test_auc * 100)` on test data.

```{r summarize-scores, echo = FALSE}
summarizeScores <- function(data, results) {
  #
  # Figure out, based on class type of the response column,
  # how to convert the response column to type factor with values
  # "success" and "failure"
  #
  tofactor <- ifelse(class(data$response) == 'integer',
    function(value) ifelse(value == 1, "success", "failure"),
    function(value) value)

data %>% select(author, title, genre, download.count, response) %>%
        inner_join(test_results) %>%
        mutate(
          author = toTitleCase(as.character(author)),
          title = tools::toTitleCase(as.character(title)),
          score = prediction,
          prediction = ifelse(score > .5, "success", "failure"),
          response = tofactor(response)) %>%
        select(author, title, score, download.count, prediction, response) %>%
        arrange(desc(score)) %>%
        mutate(score = sprintf("%02.2f", score * 100))
}

prettyPrintScores <- function(data, results) {
  kable(summarizeScores(data, results) %>% head(n = 100)) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
}
```

# Some Test Scores for the Fiction Genre
Let's look at the relative scores for examples of fiction in the test set just to get a sense for how it performs.
```{r test-scores, echo = FALSE}
prettyPrintScores(test %>% filter(genre == 'fiction'), test_results)
```
# Define function to evaluate performance
```{r define-evalauate-performance-function}
evaluatePerformance <- function(data, results) {
  tofactor <- ifelse(class(data$response) == 'integer',
    function(value) ifelse(value == 1, "success", "failure"),
    function(value) value)
  data %>% select(author, title, genre, download.count, response) %>%
      inner_join(results) %>%
      mutate(
        author = toTitleCase(as.character(author)),
        title = tools::toTitleCase(as.character(title)),
        score = prediction,
        prediction = ifelse(score > .5, "success", "failure"),
        response = tofactor(response)
      ) %>%
      createConfusionMatrix %>%
      addInformationRetrievalMetrics
}
```

# Classifier Results by Genre
This table shows how the classifier performs for a given genre.
```{r, results-by-genre}
genres <- levels(novels$genre)
resultsByGenre <- cbind(data.frame(Genre = c('all')), evaluatePerformance(test, test_results))
for(g in genres) {
  resultsByGenre <- rbind(
    resultsByGenre,
    cbind(
      data.frame(Genre = c(g)),
      evaluatePerformance(test %>% filter(genre == g), test_results)
    )
  )
}
resultsByGenre %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```
# Comparison of text2vec performance versus the best of SunnyBrook
This table shows how the GLMNET / text2vec classifier stacks up to the results reported in the "*Success with Style*" article. The GLMNET column shows the performance of our classifier. The best per genre column shows the best result obtained by each of Sunnybrook's 15 models in each genre. The best overall column shows the performance of the best single classifier over all the genres.

In three genres, viz. fiction, poetry and short-stories, the GLMNET / text2vec classify has noticably poorer results.
```{r best-of-sunnybrook, echo = FALSE}

# c('all', 'adventure', 'fiction', 'historical.fiction', 'love.story', 'mystery', 'poetry', 'science.fiction', 'short.stories')
sunnyBrookAccuracybyGenre <- data.frame(
  Genre = append(c('all'), levels(novels$genre)),
  BestPerGenre = c(73.5, 84.0, 75.0, 61.0, 82.0, 75.0, 74.0, 76.0, 77.0),
  BestOverall = c(73.5, 75.0, 75.0, 58.0, 81.0, 75.0, 72.0, 76.0, 77.0)

)
resultsByGenre %>% inner_join(sunnyBrookAccuracybyGenre, by = 'Genre') %>% mutate(GLMNET = Accuracy) %>% select(Genre, GLMNET, BestPerGenre, BestOverall) %>% kable %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))

```
